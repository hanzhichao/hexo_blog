---
layout: post
title: 简单爬虫框架分享
comments: true
toc: true
date: 2018-02-01 19:28:48
updated: 2018-02-01 19:28:48
categories:
tags: 爬虫
---
项目结构


config_load.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import platform
# handle the differences between python2 and python3
if (platform.python_version()) < '3':
    import ConfigParser
    import codecs
else:
    from configparser import ConfigParser, RawConfigParser, NoOptionError, NoSectionError
DEFAULT_CONF = os.path.join(os.path.dirname(__file__), 'conf/spider.conf')
DEFAULT_SECTION = 'spider'
class Config:
    def __init__(self, config_file_path=DEFAULT_CONF):
        try:
            if (platform.python_version()) < '3':
                # python 2
                self.cf = ConfigParser.ConfigParser()
                with codecs.open(config_file_path, encoding='utf-8-sig') as f:
                    self.cf.readfp(f)
            else:
                # python3
                # self.cf = RawConfigParser()
                # self.cf.read(config_file_path, encoding='utf8')
                self.cf = ConfigParser()
                self.cf.read(config_file_path)
        except IOError:
            raise IOError
    def get(self, option, section=DEFAULT_SECTION):
        """ get option from the config file"""
        return self.cf.get(section, option)

crawl_thread.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import platform
import time
from threading import Thread, Lock
import webpage_parser
import log
# handle the differences between python2 and python3
if (platform.python_version()) < '3':
    import Queue
else:
    import queue as Queue
# store the urls which had been crawled and saved
visited_url_list = []
url_queue = Queue.Queue(maxsize=-1)
sub_url_queue = Queue.Queue(maxsize=-1)
visited_url_list_lock = Lock()  # lock the visited_url_list while appending new url
save_page_lock = Lock()
def muti_crawl(thread_count, reg, crawl_interval, crawl_timeout, output_dir):
    """
    run multiple threads to run crawl(reg, output_dir)
    :param thread_count: type:int thread number read from config file
    :param reg: regex pattern for retrieving urls
    :param crawl_interval: threads waiting time
    :param crawl_timeout: threads timeout time
    :param output_dir: html page output dictionary
    :return: None
    """
    
    # new thread list
    threads = []
    
    # create threads
    for i in range(thread_count):
        t = Thread(target=crawl, args=(reg, output_dir))
        threads.append(t)
    
    # start threads
    for i in range(thread_count):
        threads[i].setDaemon(True)
        threads[i].start()
        time.sleep(crawl_interval)  # waiting time
    
    # waiting all threads to complete
    for i in range(thread_count):
        threads[i].join(crawl_timeout)
def crawl(reg, output_dir):
    """
    get url form the working queue, retrieve new urls and put to another queue then save the html page to output_dir
    :param reg: type:str retrieve url regex pattern
    :param output_dir: type:str read from config file
    :return: None
    """
    while not url_queue.empty():
        # get one url from url_queue
        current_url = url_queue.get()
        
        # retrieve urls from current_url using reg as regex pattern
        urls = webpage_parser.retrieve_urls(current_url, reg)
        
        # save current_url as html page in output_dir
        save_page_lock.acquire()
        webpage_parser.save_page(current_url, output_dir)
        save_page_lock.release()
        
        # lock the visited_url_list while appending new url
        visited_url_list_lock.acquire()
        visited_url_list.append(current_url)
        visited_url_list_lock.release()
        
        # if current_url is valid and has urls
        if urls:
            log.logger.debug("current url: %s" % current_url)
            for url in urls:
                if url not in visited_url_list:
                    # put new urls to new queue if url is not visited
                    sub_url_queue.put(url)


log.py
# !/usr/bin/env python
# -*- coding=utf-8 -*-
import logging
import time
import os
DEFAULT_LOG_FILE_PRX = 'spider_'
log_dir = os.path.join(os.path.dirname(__file__),'log')
# def log_config(log_dir, output_log_lever, console_log_lever):
date = time.strftime('%Y%m%d', time.localtime(time.time()))
log_file = os.path.join(log_dir, DEFAULT_LOG_FILE_PRX + date + ".log")
logger = logging.getLogger("mylogger")
logger.setLevel(logging.DEBUG)
fh = logging.FileHandler(log_file, mode='a')
fh.setLevel(logging.DEBUG)   # output log_level
ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)   # console log_level
formatter = logging.Formatter("%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s")
fh.setFormatter(formatter)
ch.setFormatter(formatter)
logger.addHandler(fh)
logger.addHandler(ch)
    
# return logger


mini_spider.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import json
import time
import os
import log
import seedfile_load
import option_parser
import config_load
import crawl_thread
def main():
    """
    main method
    :return: None
    """
    # handle command options
    if option_parser.options.conf:
        config = config_load.Config(option_parser.options.conf)
    else:
        config = config_load.Config()
    
    if option_parser.options.version:
        project_info_file = os.path.join(os.path.dirname(__file__), ".project_info.json")
        with open(project_info_file, "r") as f:
            print(json.load(f)['version'])
        exit()
    # get config options
    reg = config.get('target_url')
    max_depth = int(config.get('max_depth'))
    crawl_interval = float(config.get('crawl_interval'))
    crawl_timeout = float(config.get('crawl_timeout'))
    
    # logging start time
    log.logger.debug("spider start at: " + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) + " ---")
    start_time = time.time()
    # iterate base_urls from seed file
    for base_url in seedfile_load.get_urls(seed_file=config.get('url_list_file')):
        
        # make dirs in output_dir for each url
        output_dir = os.path.join(config.get('output_directory'), *base_url.split('/')[2:])
        log.logger.debug("output_dir: " + output_dir)
        if not os.path.exists(output_dir):
            try:
                os.makedirs(output_dir)
            except IOError:
                log.logger.error("make out_put dir error")
                
        crawl_thread.url_queue.put(base_url)   # add base_url to the url_queue of crawl_thread moudle
        log.logger.debug("base url: %s" % base_url)
        log.logger.debug("max depth: " + str(max_depth))
        
        # BFs-breadth first, consume all urls of url_queue,put new producing urls to sub_url_queue
        # and then redirect sub_url_queue to url_queue
        for current_depth in range(0, max_depth):
            log.logger.debug("current depth: %s" % current_depth)
            log.logger.debug("current queue size: %d" % crawl_thread.url_queue.qsize())
            crawl_thread.muti_crawl(thread_count=int(config.get('thread_count')),
                                    reg=reg,
                                    crawl_interval=crawl_interval,
                                    crawl_timeout=crawl_timeout,
                                    output_dir=output_dir)
            
            # make sub_url_queue the working queue
            crawl_thread.url_queue = crawl_thread.sub_url_queue
            current_depth += 1
    
    # logging end time and full duration
    log.logger.debug("spider gracefully end at: " + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) +
                     " Full Duration: " + str(time.time() - start_time) + 's' + " ---")
main()


option_parser.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import optparse
# comand option parser for mini_spider.py
parser = optparse.OptionParser()
parser.add_option("-c", "--conf", action="store", dest="conf", help="load config file", metavar="FILE")
parser.add_option("-v", "--version", action="store_true", dest="version", help="show project version")
(options, args) = parser.parse_args()


seedfile_load.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
def get_urls(seed_file):
    """
    get urls from seed file,a new blank line needed at the end of the file
    @:param seed_file type:str seed_file path, read from the config file
    """
    with open(seed_file, 'r') as f:
        # add 'http://'; if url not contains 'http'
        return map(lambda x: 'http://'; + x[:-1] if 'http' not in x else x[:-1], f.readlines())


webpage_parser.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re
import os
import platform
import log
# handle the difference between python2 and python3
if (platform.python_version()) < '3':
    import urllib
    import HTMLParser
else:
    import urllib.request as urllib
    import html.parser as HTMLParser
class UrlParser(HTMLParser.HTMLParser):
    def __init__(self):
        self.links = []
        HTMLParser.HTMLParser.__init__(self)
    
    def handle_starttag(self, tag, attrs):
        if tag == 'a':
            for name, value in attrs:
                if name == 'href':
                    self.links.append(value)
    
    def get_links(self):
        return self.links
def retrieve_urls(current_url, pattern):
    """
    retrieve urls from url using regex pattern
    @:param url type:str should contains 'http://';
    @:param reg type:str regex pattern read from spider.conf
    :return url_list(type:list) if current_url is valid
    :return None if url is invalid
    """
    parser = UrlParser()
    try:
        parser.feed(urllib.urlopen(current_url).read())
        links = parser.get_links()
        # log.logger.debug("links: " + ','.join(links))
        reg_pattern = re.compile(pattern)
        # log.logger.debug('reg pattern: ' + pattern)
        url_list = []
        for link in links:
            # log.logger.debug('link: ' + link)
            match = re.match(reg_pattern, link)
            
            if match:
                # log.logger.debug('match result: ' + match.group())
                url_list.append(match.group())
            else:
                pass
                # log.logger.debug('match result: ' + "None")
            
        url_list = list(set(url_list))  # remove duplicated urls
        # log.logger.debug("url_list: " + ",".join(url_list))
        
        # if urls in url_list not contains 'http' then add current_url before
        # if urls contains 'javascript' add current_url and the last part
        def format_url(url):
            if "&quot;" in url:
                url = url.replace("&quot;", '"')
            if "&nbsp;" in url:
                url = url.replace("&nbsp;", " ")
                
            if 'http' in url:
                return url
            elif url[:2] == '//':
                return 'http:' + url
            else:
                base_url = '/'.join(current_url.split('/')[:-1])
                if 'javascript' in url:
                    if '=' in url:
                        try:
                            return base_url + '/' + url.split('=')[1][1:]
                        except IndexError:
                            log.logger.warning("url: %s format fail" % url)
                    else:
                        return None
                else:
                    return base_url + '/' + url
        url_list = map(format_url, url_list)
        
        return url_list
    except IOError:
        log.logger.warning("current url: %s is invalid" % current_url)
        return None
def save_page(url, output_dir):
    """
    save url html page to output_dir
    :param url: url to save, should contains .html, will makedirs if uri contains muti dirs
    :param output_dir: base output dir
    :return: None
    """
    if output_dir[0] != '/':
        if output_dir[:2] == './':
            output_dir = output_dir[2:]
        output_dir = os.path.join(os.path.dirname(__file__), output_dir)
    log.logger.debug("url: " + url)
    uri = url.split('/')[3:]
    log.logger.debug("uri: " + ','.join(uri))
    if uri:
        file_dir = os.path.join(output_dir, *uri[:-1])  # eg. ./output/page1/page1_1
        file_name = uri[-1]
    else:
        file_dir = output_dir
        file_name = 'index.html'
    
    log.logger.debug("file_dir: " + file_dir)
    if not os.path.exists(file_dir):
        if 'Windows' in platform.platform():
            try:
                os.makedirs(file_dir)  # make sub dirs in output dir
            except WindowsError:
                log.logger.error("create dirs: %s fail", file_dir)
        else:
            try:
                os.makedirs(file_dir)  # make sub dirs in output dir
            except OSError:
                log.logger.error("create dirs: %s fail", file_dir)
            
    file_path = os.path.join(file_dir, file_name)  # eg. page1_1_1.html
    log.logger.debug("file_path: " + file_path)
    try:
        urllib.urlretrieve(url, file_path)  # save html
    except IOError:
        log.logger.error("save file: %s fail" % file_path)


conf/spider.conf
[spider]
url_list_file: ./seed/urls ; 种子文件路径
output_directory: ./output ; 抓取结果存储目录
max_depth: 3 ; 最大抓取深度(种子为0级)
crawl_interval: 1 ; 抓取间隔. 单位: 秒
crawl_timeout: 1 ; 抓取超时. 单位: 秒
target_url: .*.html|htm ; 需要存储的目标网页URL pattern(正则表达式)
thread_count: 8 ; 抓取线程数


seed/urls
www.baidu.com
www.sina.com


.project_info.json
{
  "project_name": "mini_spider",
  "author": "Li Dan",
  "version": "0.1",
  "last_update": "2018/01/13"
}

test/test_config.py


test/test_crawl_thread.py


test/test_option_parser.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import unittest
import json
import subprocess
class TestOptionParser(unittest.TestCase):
    def setUp(self):
        pass
    
    def tearDown(self):
        pass
    
    @staticmethod
    def cmd(cmd):
        mytask = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE,
                                  stderr=subprocess.STDOUT)
    
        stdstr = mytask.stdout.read()
        return stdstr
    
    def test_option_help(self):
        self.assertTrue('show this help message and exit' in self.cmd('python ../mini_spider.py -h'))
        self.assertTrue('show this help message and exit' in self.cmd('python ../mini_spider.py --help'))
    
    def test_option_version(self):
        project_info_file = os.path.join(os.path.dirname(__file__), "../.project_info.json")
        with open(project_info_file, "r") as f:
            version = json.load(f)['version']
        self.assertTrue(version in self.cmd('python ../mini_spider.py -v'))
        self.assertTrue(version in self.cmd('python ../mini_spider.py --version'))
    
    def test_option_conf(self):
        pass
    
    def test_option_conf_no_args(self):
        self.assertTrue('option requires 1 argument' in self.cmd('python ../mini_spider.py -c'))
        self.assertTrue('option requires 1 argument' in self.cmd('python ../mini_spider.py --conf'))
        
    def test_config_file_not_exist(self):
        pass


test_seedfile_load.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import unittest
import os
class TestSeedFileLoad(unittest.TestCase):
    def setUp(self):
        self.seed_file = os.path.join(os.path.dirname(__file__), '../seed/urls')
        self.origin_content = self.read_seed_file()
    
    def tearDown(self):
        self.write_seed_file(self.origin_content)
    
    def read_seed_file(self):
        with open(self.seed_file) as f:
            return f.read()
        
    def write_seed_file(self, content):
        with open(self.seed_file, 'w') as f:
            f.write(self.origin_content)
            
    def test_muti_lines(self):
        pass
    
    def test_no_new_line_end(self):
        pass
    
    def test_url_contains_http(self):
        pass
    
    def test_seed_file_not_exists(self):
        pass


test_webpage_parser.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import unittest
import webpage_parser
class TestSeedFileLoad(unittest.TestCase):
    def setUp(self):
        pass
    
    def tearDown(self):
        pass
    @staticmethod
    def format_url(current_url, url):
        if "&quot;" in url:
            url = url.replace("&quot;", '"')
        if "&nbsp;" in url:
            url = url.replace("&nbsp;", " ")
    
        if 'http' in url:
            return url
        elif url[:2] == '//':
            return 'http:' + url
        elif 'javascript' in url:
            if '=' in url:
                try:
                    return current_url + '/' + url.split('=')[1][1:]
                except IndexError:
                    print("url: %s format fail" % url)
            else:
                return None
        else:
            return current_url + '/' + url
    
    def test_save_html(self):
        webpage_parser.save_page('pycm.baidu.com:8081/page1.html', './output')
        try:
            with open('../output/baidu.html', 'r') as f:
                html = f.read()
            self.assertTrue('page1.html' in html)
        except IOError:
            self.fail('no html fail saved')
    def test_save_html_with_dirs(self):
        webpage_parser.save_page('pycm.baidu.com:8081/page1/page1_1.html', 'output')
        try:
            with open('../output/page1/page1_1.html', 'r') as f:
                html = f.read()
            self.assertTrue('page1_1' in html)
        except IOError:
            self.fail('no html fail saved')
    def test_format_url(self):
        self.assertEqual(self.format_url('http://www.baidu.com';, '//www.baidu.com/cache/sethelp/help.html';),
                         'http://www.baidu.com/cache/sethelp/help.html';)
        self.assertEqual(self.format_url('http://www.baidu.com';, 'http://www.baidu.com/cache/sethelp/help.html';),
                         'http://www.baidu.com/cache/sethelp/help.html';)
        self.assertEqual(self.format_url('http://www.baidu.com';, 'https://www.baidu.com';),
                         'https://www.baidu.com';)
        self.assertEqual(self.format_url('http://www.baidu.com';, 'javascript;'),
                         None)
        self.assertEqual(self.format_url('http://www.baidu.com';, 'javascript:location.href=&quot;page4.html'),
                         'http://www.baidu.com/page4.html';)
        self.assertEqual(self.format_url('http://www.baidu.com';, 'page4.html'),
                         'http://www.baidu.com/page4.html';)
        
    def test_retrieve_urls(self):
        url_list = webpage_parser.retrieve_urls('http://www.baidu.com';, '.*.html|htm')
        self.assertTrue('http://www.baidu.com/gaoji/preferences.html'; in url_list)
        # print url_list
        url_list = webpage_parser.retrieve_urls('http://www.baidu.com/gaoji/preferences.html';, '.*.html|htm')
        self.assertTrue(url_list == [])
        url_list = webpage_parser.retrieve_urls('http://www.baidu.com/cache/sethelp/help.html';, '.*.html|htm')
        # print url_list
        self.assertTrue('http://www.baidu.com/duty/index.html'; in url_list)
        url_list = webpage_parser.retrieve_urls('http://www.baidu.com/duty/index.html';, '.*.html|htm')
        print url_list
        self.assertTrue('http://www.baidu.com/duty/yinsiquan.html'; in url_list)

